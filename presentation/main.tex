\documentclass[10pt]{beamer}
\usetheme{metropolis}
% all imports
\input{all_imports}

\AtBeginEnvironment{quote}{\singlespacing}

% new commands
\input{all_new_commands}

% definitions
\input{definitions/colors}
\input{definitions/styles}

\input{header}

\begin{document}


\maketitle


\begin{frame}{Research problem}
\begin{itemize}
\item Create a set of tasks that incorporate logic reasoning to boost performance of the current dialog agents.
\item Perform a stress test in the existing \textit{neural network based end-to-end dialog systems}.
\item Integrate linguistic reasoning with visual references to create a new set of visual question answering (VQA) tasks.
\item Define new models to achieve better results in the tasks proposed above. 
\end{itemize}
\end{frame}


\section{Background}

\begin{frame}{Neural network based language model}
We call \alert{language model} a probability distribution over sequences of tokens in a natural language.
\begin{equation}
P(x_1,x_2,x_3,x_4) = p
\end{equation}

Since \cite{Mikolov11}, we use a \alert{Recurrent Neural Network (RNN)} to estimate the probability distribution   \\

\begin{equation}
P(x_{n} = \text{word}_{j^{*}} | x_{1}, \dots ,x_{n-1})
\end{equation}

for any $(n-1)$-sequence of words $x_{1}, \dots ,x_{n-1}$.
\end{frame}

\begin{frame}{Neural network based language model}
\input{tikzfiles/LanguageModelUnfolded}
\end{frame}



\begin{frame}{GRU: Gated Recurrent Units}
\begin{equation}
\vect{\widetilde{h}}^{(t)} = tahn(\vect{W} (\vect{h}^{(t-1)} \odot  \vect{r}^{(t)}) + \vect{U} \vect{x}^{(t)} + \vect{b})
\end{equation}

\begin{equation}
\vect{r}^{(t)} = \sigma(\vect{W}_{r} \vect{h}^{(t-1)} + \vect{U}_{r} \vect{x}^{(t)} + \vect{b}_{r})
\end{equation}


\begin{equation}
\vect{u}^{(t)} = \sigma(\vect{W}_{u} \vect{h}^{(t-1)} + \vect{U}_{u} \vect{x}^{(t)} + \vect{b}_{u})
\end{equation}

\begin{equation}
\vect{h}^{(t)} = \vect{u}^{(t)} \odot \vect{\widetilde{h}}^{(t)} + (1 - \vect{u}^{(t)}) \odot \vect{h}^{(t-1)} 
\end{equation}

\end{frame}

\begin{frame}{LSTM: Long Short Term Memory}
\begin{equation}
\vect{f}^{(t)} = \sigma(\vect{W}_{f} \vect{h}^{(t-1)} + \vect{U}_{f} \vect{x}^{(t)} + \vect{b}_{f})
\end{equation}

\begin{equation}
\vect{i}^{(t)} = \sigma(\vect{W}_{i} \vect{h}^{(t-1)} + \vect{U}_{i} \vect{x}^{(t)} + \vect{b}_{i})
\end{equation}

\begin{equation}
\vect{o}^{(t)} = \sigma(\vect{W}_{o} \vect{h}^{(t-1)} + \vect{U}_{o} \vect{x}^{(t)} + \vect{b}_{o})
\end{equation}


\begin{equation}
\tilde{\vect{c}}^{(t)} = tahn(\vect{W} \vect{h}^{(t-1)} + \vect{U} \vect{x}^{(t)} + \vect{b})
\end{equation}

\begin{equation}
\vect{c}^{(t)} = \vect{f}^{(t)} \odot \vect{c}^{(t-1)} + \vect{i}^{(t)} \odot \tilde{\vect{c}}^{(t)}
\end{equation}

\begin{equation}
\vect{h}^{(t)} = \vect{o}^{(t)} \odot tanh(\vect{c}^{(t)})
\end{equation}

\end{frame}

\begin{frame}{Sequence-to-sequence}
\begin{itemize}
\item $\vect{x}^{(1)}, \dots, \vect{x}^{(n)}$, source sentence
\item $\vect{y}^{(1)}, \dots, \vect{y}^{(m)}$, target sentence
\item $f_{enc}$ (the \textit{encoder}), a RNN
\item $f_{dec}$ (the \textit{encoder}), a language model
\end{itemize}


\begin{equation}
\vect{s} = f_{enc}(\vect{x}^{(n)}, \vect{h}^{(n-1)})
\end{equation}

\begin{equation}
\vect{\tilde{h}}^{(t)} = f_{dec}(\vect{y}^{(t)}, \vect{\tilde{h}}^{(t-1)})
\end{equation}

\begin{equation}
p(y_t | y_1, \dots, y_{t-1}, x_1, \dots, x_{n}) = softmax(\vect{W}_{s}  \vect{\tilde{h}}^{(t)} + \vect{b}_s)
\end{equation}


\end{frame}

\begin{frame}{Attention}

\begin{equation}
\vect{a}_{ts} = \frac{exp(score(\vect{\tilde{h}}^{(t)},\vect{h}^{(s)}))}{\sum_j exp(score(\vect{\tilde{h}}^{(t)},\vect{h}^{(j)}))}
\end{equation}

\begin{equation}
score(\vect{\tilde{h}}^{(t)},\vect{h}^{(s)}) = \begin{cases}
{\vect{\tilde{h}}^{(t)}} .^{\top} \vect{h}^{(s)}\\
\vect{\tilde{h}}^{(t)} .^{\top}\vect{W}_a \vect{h}^{(s)}\\
\vect{v}_a ^{\top}tahn(\vect{W}_a[\vect{\tilde{h}}^{(t)};\vect{h}^{(s)}])\\
\end{cases}
\end{equation}

\begin{equation}
\vect{c}^{(t)} = \sum_{s} \vect{a}_{ts}\vect{h}^{(s)}
\end{equation}

\begin{equation}
\vect{\tilde{h}}^{(t)}_{out} = tahn(\vect{W}_c[\vect{c}^{(t)};\vect{h}^{(t)}])
\end{equation}

\begin{equation}
p(y_t | y_1, \dots, y_{t-1}, x_1, \dots, x_{n}) = softmax(\vect{W}_{s}  \vect{\tilde{h}}^{(t)}_{out} + \vect{b}_s)
\end{equation}

\end{frame}


\section{Neural network based dialog systems}

\begin{frame}{Seq2seq applied to translation}
\input{tikzfiles/Translation}
\end{frame}

\begin{frame}{Seq2seq applied to dialog \cite{DBLP:journals/corr/VinyalsL15}}
\input{tikzfiles/seq2seq_dialog}
\end{frame}


\begin{frame}{MemNN}
\begin{itemize}
\item $U_1, \dots, U_n$ context
\item $q$ question
\item $a$ answer
\end{itemize}

We have $k = 1, \dots, K$ memory layers: 

\begin{itemize}
\item $\{ {\vect{m}^{(k)}}_i\}$, memory vectors
\item $\vect{u}^{(k)}$, input vector
\item $\vect{p}^{(k)}$,  match between $\vect{u}^{(k)}$ and each $\vect{m}^{(k)}_i$
\item $\{ {\vect{c}^{(k)}}_i\}$, another representation of the context $U_1, ..., U_n$
\item $\vect{o}^{(k)}$, output.
\item $\vect{\hat{a}}= softmax(\vect{W}(\vect{o}^{K}))$, candidate answer
\end{itemize}
\end{frame}



\section{How to evaluate dialogs?}


\begin{frame}{Human evaluation  \cite{Lowe:2016}}
\begin{center}
\includegraphics[scale=0.2]{images/exemploEval1.png}
\end{center}

\begin{enumerate}
\item \alert{Adequacy}: the meaning equivalence between the generated and control sentence. 
\item \alert{Fluency}: the syntactic correctness of the generated sequence.
\item \alert{Readability}: efficacy of the generated sentence in a particular context.
\end{enumerate}

\end{frame}

\begin{frame}{BLEU (bilingual evaluation understudy)}
\begin{equation}
P_n = \frac{\text{number of } n\text{-grams in both } \hat{y} \text{ and } y}{\text{number of } n\text{-grams appearing in } \hat{y}}
\end{equation}    
\vspace{0.2cm}

\begin{equation}
BP=
\begin{cases}
1 & \text{if } len(\hat{y}) > len(y) \\
\exp\left( 1 - \frac{len(y)}{len(\hat{y})} \right) & \text{otherwise}
\end{cases}
\end{equation} 
\vspace{0.2cm}

\begin{equation}
BLEU = BP \; \exp \left(\frac{1}{N}  \sum_{n=1}^{N} \log P_n \right)
\end{equation}
\end{frame}


\begin{frame}{METEOR (Metric for Evaluation of Translation with Explicit ORdering)}
\begin{equation}
P = \frac{\text{number of } \text{unigrams in both } \hat{y} \text{ and } y}{\text{number of } \text{unigrams appearing in } \hat{y}}
\end{equation}    


\begin{equation}
R = \frac{\text{number of } \text{unigrams in both } \hat{y} \text{ and } y}{\text{number of } \text{unigrams appearing in } y}
\end{equation}    


\begin{equation}
F_{mean} = \frac{10 P R}{R + 9P}
\end{equation}

\begin{equation}
METEOR = F_{mean} (1 - penalty)
\end{equation}
\end{frame}

\begin{frame}{ROUGE (Recall Oriented Understudy for Gisting Evaluation)}
\begin{equation}
P_{lcs} = \frac{lcs(\hat{y}, y)}{len(\hat{y})}
\end{equation}    


\begin{equation}
R_{lcs} = \frac{lcs(\hat{y}, y)}{len(y)}
\end{equation}

\begin{equation}
ROUGE_L = \frac{(1 + \beta^2) P_{lcs} R_{lcs}}{R_{lcs} + \beta^{2}P_{lcs}}
\end{equation}

where $\beta$ is usually set to favour recal ($\beta = 1.2$).
\end{frame}


\begin{frame}{Problems \cite{LiuLSNCP16}}

\begin{table}[h]
\centering
\label{hownottable}
\begin{tabular}{|c|c|c|c|c|}
\hline
\cellcolor{blue!50} metric & \cellcolor{blue!50} Spearman & \cellcolor{blue!50} $p$-value & \cellcolor{blue!50} Pearson &  \cellcolor{blue!50} $p$-value \\ \hline
BLEU   & $0.34$   & $< 0.01$  & $0.14$  & $0.17$ \\ \hline
METEOR & $0.19$   & $0.06$    & $0.19$  & $0.05$ \\ \hline
ROUGE  & $0.12$   & $0.22$    & $0.1$   & $0.34$ \\ \hline  
\end{tabular}
\caption{Correlation between automatic metrics and human judgments based on dialog generated on Twitter}
\end{table}

\begin{table}[h]
\centering
\label{hownottable}
\begin{tabular}{|c|c|c|c|c|}
\hline
\cellcolor{blue!50} metric & \cellcolor{blue!50} Spearman & \cellcolor{blue!50} $p$-value & \cellcolor{blue!50} Pearson &  \cellcolor{blue!50} $p$-value \\ \hline
BLEU & $0.12$   & $0.23$    & $0.11$  & $0.26$    \\ \hline
METEOR & $0.06$   & $0.53$    & $0.14$  & $0.16$     \\ \hline
ROUGE & $0.05$   & $0.59$    & $0.06$  & $0.53$    \\ \hline
\end{tabular}
\caption{Correlation between automatic metrics and human judgments based on dialog generated on Ubuntu}
\end{table}
\end{frame}

\section{Creating simplified tasks as tests}

\begin{frame}{bAbI \cite{WestonBCM15}}
One solution is to create a set of QA synthetic tasks to test different capabilities of a dialog agent.


\begin{center}
\includegraphics[scale=0.25]{images/babi.png}
\end{center}
\end{frame}



\begin{frame}{ParlAI \\ \url{https://github.com/facebookresearch/ParlAI}}

\begin{center}
\includegraphics[scale=0.84]{images/parlai.png}
\end{center}

"ParlAI (pronounced 'par-lay') is a framework for dialog AI research, implemented in Python.

Its goal is to provide researchers:

\begin{itemize}
\item a unified framework for sharing, training and testing dialog models
\item many popular datasets available all in one place, with the ability to multi-task over them
\item seamless integration of Amazon Mechanical Turk for data collection and human evaluation"
\end{itemize}

\end{frame}

\begin{frame}{Sanity check experiments}
\begin{center}
\includegraphics[scale=0.34]{images/comparative_results_babi1.png}
\end{center}
\end{frame}

\begin{frame}{Sanity check experiments}
\begin{center}
\includegraphics[scale=0.34]{images/comparative_results_babi2.png}
\end{center}
\end{frame}

\section{Entailment-QA}

\begin{frame}{bAbI: task 15}

\alert{Basic Deduction}

\begin{center}
\includegraphics[scale=0.28]{images/babi15.png}
\end{center}
\begin{quote} 
\centering 
$P^{1}$ are afraid of $Q^{1}$\\
$P^{2}$ are afraid of $Q^{2}$\\
$P^{3}$ are afraid of $Q^{3}$\\
$P^{4}$ are afraid of $Q^{4}$\\
$c^{1}$ is a $P^{1}$\\
$c^{2}$ is a $P^{2}$\\
$c^{3}$ is a $P^{3}$\\
$c^{4}$ is a $P^{4}$\\
What is $c^j$ afraid of? \alert{A: $Q^j$}\\
\end{quote}


\end{frame}


\begin{frame}{bAbI: task 16}

\alert{Basic Induction}

\begin{center}
\includegraphics[scale=0.28]{images/babi16.png}
\end{center}
\begin{quote} 
\centering 
$c^{1}$ is a $P^{1}$\\
$c^{1}$ is $C^{1}$\\
$c^{2}$ is a $P^{2}$\\
$c^{2}$ is $C^{2}$\\
$c^{3}$ is a $P^{3}$\\
$c^{3}$ is $C^{3}$\\
$c^{4}$ is a $P^{4}$\\
$c^{4}$ is $C^{4}$\\
$c$ is a $P^{j}$\\
What color is $c$? \alert{A: $C^j$}\\
\end{quote}

\end{frame}




\begin{frame}{Entailment-QA}

\begin{enumerate}
\item \textbf{Boolean Connectives}
\item[]
\item \textbf{First-Order Quantifiers}
\item[]
\item \textbf{Synonymy}
\item[]
\item \textbf{Antinomy}
\item[]
\item \textbf{Hypernymy}
\item[]
\item \textbf{Active/Passive voice}
\end{enumerate}
\end{frame}

\begin{frame}{Entailment-QA: task 1}
\begin{itemize}
\item \alert{Entailment} ($s_1$ implies $s_2$)
\begin{itemize}
\item $\underbrace{P^{1}a^1 \land \dots \land P^{n}a^n}_{s_1}, \underbrace{P^{j}a^j}_{s_2}$ 
\item $\underbrace{P^{j}a^j}_{s_1}, \underbrace{P^{1}a^1 \lor \dots \lor P^{n}a^n}_{s_2}$
\item $\underbrace{Pa}_{s_1}, \underbrace{\lnot \lnot Pa}_{s_2}$
\end{itemize}

\vspace{0.4cm}
\item \alert{Not entailment} ($s_1$ does not imply $s_2$)
\begin{itemize}
\item $\underbrace{P^{j}a^j}_{s_1}, \underbrace{P^{1}a^1 \land \dots \land P^{n}a^n}_{s_2}$ 
\item $\underbrace{P^{1}a^1 \lor \dots \lor P^{n}a^n}_{s_1}, \underbrace{P^{j}a^j}_{s_2}$
\item $\underbrace{Pa}_{s_1}, \underbrace{\lnot Pa}_{s_2}$
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Entailment-QA: task 1}
\begin{itemize} 
\item[] Ashley is fit
\item[] Ashley is not fit
\item[] The first sentence implies the second sentence? \alert{A: no}
\end{itemize}

\vspace{0.3cm}


\begin{itemize} 
\item[]Avery is nice and Avery is obedient
\item[]Avery is nice
\item[]The first sentence implies the second sentence? \alert{A: yes}
\end{itemize}

\vspace{0.3cm}

\begin{itemize} 
\item[]Elbert is handsome or Elbert is long
\item[]Elbert is handsome
\item[]The first sentence implies the second sentence? \alert{A: no}
\end{itemize}
\end{frame}


\begin{frame}{Entailment-QA: task 2}

\begin{itemize}
\item \alert{Entailment}
\begin{itemize}
\item $\forall x Px, Pa$ 
\item $Pa, \exists x Px$ 
\end{itemize}
\item []
\item \alert{Contradiction}
\begin{itemize}
\item $\forall x Px, \lnot Pa$ 
\item $\forall x Px, \exists x \lnot Px$ 
\end{itemize}
\item []
\item \alert{Neutral}
\begin{itemize}
\item $Pa,Qa$ 
\item $\forall x Px, \lnot Qa$ 
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Entailment-QA: task 2}

\begin{itemize} 
\item[] Every person is lively
\item[] Belden is lively
\item[] What is the semantic relation? \alert{A: entailment}
\end{itemize}

\begin{itemize} 
\item[] Every person is short
\item[] There is one person that is not short
\item[] What is the semantic relation?  \alert{A: contradiction}
\end{itemize}

\begin{itemize} 
\item[] Every person is beautiful
\item[] Abilene is not blue
\item[] What is the semantic relation? \alert{A: neutral}
\end{itemize}
\end{frame}

\begin{frame}{Entailment-QA: task proxy}

SICK (Sentences Involving Compositional Knowledge) \cite{Marelli14}

\begin{center}
\includegraphics[scale=0.25]{images/sick.png}
\end{center}

\end{frame}


\begin{frame}{Entailment-QA: task proxy}

\begin{itemize} 
\item[] There is no dog leaping in the air
\item[] A dog is leaping high in the air and another is watching
\item[] What is the semantic relation? \alert{A: contradiction}
\end{itemize}

\begin{itemize} 
\item[] A man is exercising
\item[] A baby is laughing
\item[] What is the semantic relation? \alert{A: neutral}
\end{itemize}

\begin{itemize} 
\item[] Some dogs are playing in a river
\item[] Some dogs are playing in a stream
\item[] What is the semantic relation? \alert{A: entailment}
\end{itemize}
\end{frame}



\begin{frame}{Preliminary Results}
\begin{center}
\includegraphics[scale=0.42]{images/comparative_results.png}
\end{center}
\end{frame}



\begin{frame}{Preliminary Results}
\begin{center}
\includegraphics[scale=0.28]{images/training_acc_EntailQA_mem.png}
\end{center}
\end{frame}


\begin{frame}{Preliminary Results}
\begin{center}
\includegraphics[scale=0.42]{images/cm_mem_EntailQA2.png}
\end{center}
\end{frame}


\begin{frame}{Future Steps}
\begin{itemize}
\item Try to overcome the reported overfitting problem. 
\item Finish the Entailment-QA corpus.
\item Explore new models not mentioned here, like \alert{Dynamic Memory Networks} \cite{KumarISBEPOGS15} and  \alert{Memory Attention and Composition (MAC) cell} \cite{Manning18}.
\item Create a visual version of the Entailment-QA to test logical inference with images.
\item Check the reinforcement learning on dialog.
\item Review the literature on the theory of comparing models \cite{BenavoliCDZ17}.
\end{itemize}
\end{frame}


\begin{frame}{Schedule}
\begin{center}
\includegraphics[scale=0.24]{images/workplan.png}
\end{center}
\end{frame}



\begin{frame}[allowframebreaks]{References}

  \bibliography{my_references}
  \bibliographystyle{abbrv}

\end{frame}

\end{document}




\end{document}