\documentclass[10pt]{beamer}
\usetheme{metropolis}
% all imports
\input{all_imports}

\AtBeginEnvironment{quote}{\singlespacing}

% new commands
\input{all_new_commands}

% definitions
\input{definitions/colors}
\input{definitions/styles}

\input{header}

\begin{document}


\maketitle


\begin{frame}{Geração de diálogo}
\begin{center}
\includegraphics[scale=0.16]{images/turing.jpg}
\end{center}
\end{frame}


\begin{frame}{Sistemas de diálogo}
\begin{itemize}
\item \alert{Goal-driven systems}:
\begin{itemize}
\item serviços de suporte técnico
\item marketing      
\item sistemas de reserva
\item sistemas de informação
\end{itemize}

\vspace{0.4cm}
\item \alert{Non-goal-driven systems}
\begin{itemize}
\item Conversas livres (sem fim específico)
\end{itemize}
\end{itemize}


\end{frame}

\section{Sistemas de diálogo baseados em redes neurais}

\begin{frame}{Modelos de linguagem baseados em redes neurais}
Nos chamamos de \alert{modelo de linguagem} uma distribuição de probabildiade sobre uma sequencia de tokens em uma lingua natural.

\[
P(x_1,x_2,x_3,x_4) = p
\]

Em vez de usar uma abordagem que seja específica para o domínio da linguagem natural, podemos usar um modelo para predição de dados sequencias:  \textbf{uma rede recorrente (RNN)}. \\

Nossa tarefa de aprendizado é estimar a distribuição de probabilidade

\[
P(x_{n} = \text{palavra}_{j^{*}} | x_{1}, \dots ,x_{n-1})
\]

para qualquer $(n-1)$ sequencia de palavras $x_{1}, \dots ,x_{n-1}$.

\end{frame}

\begin{frame}{O modelo de linguagem com RNN}
\input{tikzfiles/LanguageModelUnfolded}
\end{frame}



\begin{frame}{GRU: Gated Recurrent Units}
\begin{center}
\includegraphics[scale=0.25]{images/gru.png}
\end{center}
\end{frame}


% \begin{frame}{LSTM: Long Short Term Memory}
% \begin{center}
% \includegraphics[scale=0.23]{images/lstm.png}
% \end{center}
% \end{frame}


\begin{frame}{Exemplo: TrumpBot\\\url{https://github.com/felipessalvatore/MyTwitterBot}}
\begin{center}
\includegraphics[scale=0.24]{images/TrumpBot.png}
\end{center}
\end{frame}


\begin{frame}{Exemplo: Funk Generator\\ \url{https://github.com/lucasmoura/funk_generator}}
\begin{quote}
\centering
É o dj que tá tocando e não sabe de nada\\ 
Eu já tô no clima e já tô no meu nome \\
Cordão de ouro no pescoço eu tô na moda \\
Com a camisa da \\
Louis \\
Vuitton \\
Pulo da morena que elas gosta\\ 
E se eu te pego no baile \\
De captiva de citroen ou de hayabusa\\ 
Tu viu a 1100 cilindradas \\
Se eu tô no litoral de cordão de ouro\\ 
De cordão de ouro no pescoço\\
\end{quote}
\end{frame}

\begin{frame}{Seq2seq: diálogo \cite{DBLP:journals/corr/VinyalsL15}}
\input{tikzfiles/seq2seq_dialog}
\end{frame}


\begin{frame}{Seq2seq: tradução \cite{luzfinger2017}}
\input{tikzfiles/Translation}
\end{frame}

\begin{frame}{Exemplo de diálogo \cite{DBLP:journals/corr/VinyalsL15}}
\begin{center}
\includegraphics[scale=0.3]{images/exemplo1.png}
\end{center}
\end{frame}


\section{Métricas}


\begin{frame}{Avaliação humana \cite{Lowe:2016}}
\begin{center}
\includegraphics[scale=0.4]{images/exemploEval1.png}
\end{center}
\end{frame}




\begin{frame}{Avaliação automática: BLEU (bilingual evaluation understudy) \cite{Papineni2001}}
Essa métrica compara n-gramas (até 4) da resposta candidata com os n-gramas da refência da tradução e conta o numero de acertos. Essa métrica também penaliza traduções muito curtas:

\begin{equation*}
BLUE(r, \hat{r}) = min \left(1, \frac{len(\hat{r})}{len(r)} \right) \left(\prod_{n=1}^{4} precision_{n}(r, \hat{r}) \right)^{\frac{1}{4}}
\end{equation*}
em que $ precision_{n}(r, \hat{r})$ é o número de overlap de $n$ gramas de $r$ e $\hat{r}$ dividido pelo número de todos os $n$-gramas de $\hat{r}$. 

\begin{itemize}
\item $BLUE(r, \hat{r}) \in [0,1]$
\end{itemize}

\end{frame}


\begin{frame}{Avaliação automática: problemas}
\begin{center}
\includegraphics[scale=0.23]{images/weak_corr.png}
\end{center}

"In particular, we show that these metrics (BLEU, METEOR, ROUGE) have only a small positive correlation on the chitchat oriented Twitter dataset, and no correlation at all on the technical Ubuntu Dialogue Corpus." \cite{LiuLSNCP16}

\end{frame}

\section{De diálogos abertos para pequenas tarefas}

\begin{frame}{bAbI \cite{WestonBCM15}}
Criar uma série de pequenas tarefas para testar diferentes capacidades de um sistema de diálogo.


\begin{center}
\includegraphics[scale=0.25]{images/babi.png}
\end{center}
\end{frame}

\begin{frame}{Seq2seq}
\begin{itemize}
\item $\vect{x}_1, \dots, \vect{x}_n$ sentença de entrada (source)
\item $\vect{y}_1, \dots, \vect{y}_m$ sentença de saida (target)
\item $\vect{s} = f_{enc}(\vect{x}_n, \vect{h}_{n-1})$ representação  até a entrada $\vect{x}_n$
\begin{itemize}
\item $\vect{h}_0$ iniciado aleatoriamemnte
\item $\vect{h}_n = \vect{s}$  
\end{itemize}
\item $\vect{h}^{\prime}_j = f_{dec}(\vect{y}_j, \vect{h}^{\prime}_{j-1})$ representação saida no instante $j$
\begin{itemize}
\item $\vect{h}^{\prime}_0 = \vect{s}$
\end{itemize}

\item $p(y_j | y_1, \dots, y_{j-1}, x_1, \dots, x_{n}) = softmax(\vect{W}_{s}  \vect{h}^{\prime}_j + \vect{b}_s)$

\item $\hat{y}_j = \argmax p(y_j | y_1, \dots, y_{j-1}, x_1, \dots, x_{n})$
\end{itemize}
\end{frame}

\begin{frame}{Seq2seq}
\input{tikzfiles/seq2seq_dialog}
\end{frame}


\begin{frame}{Modelo de atenção}
 No modelo de atenção vamos contruir um vetor de contexto $\vect{c}_t$ para selecionar as informações na sentença de entrada.\\
 Uma vez costruido $\vect{c}_t$, definimos um estado de atenção

\begin{equation*}
\vect{\tilde{h}}_t = tahn(\vect{W}_c[\vect{c}_t;\vect{h}_t])
\end{equation*}


 e geramos a predição

\begin{equation*}
p(y_t | y_1, \dots, y_{t-1}, x_1, \dots, x_{n}) = softmax(\vect{W}_{s}  \vect{\tilde{h}}_t + \vect{b}_s)
\end{equation*}
\end{frame}


\begin{frame}{Como definimos $\vect{c}_t$?}
Construimos uma matrix de alinhamento $a$ tal que

\begin{itemize}
\item $a_{ts}$ é a probabilidade da representação de entrada $\vect{h}_s$ ser relevante para a saída $\hat{y}_t$.
\item[]
\item $a_{ts} = \frac{exp(score(\vect{\tilde{h}}_t,\vect{h}_s))}{\sum_j exp(score(\vect{\tilde{h}}_t,\vect{h}_j))}$ 

\item $score(\vect{\tilde{h}}_t,\vect{h}_s) = \begin{cases}
\vect{\tilde{h}}_t ^{\top}\vect{h}_s\\
\vect{\tilde{h}}_t ^{\top}\vect{W}_a \vect{h}_s\\
\vect{v}_a ^{\top}tahn(\vect{W}_a[\vect{\tilde{h}}_t;\vect{h}_s])\\
\end{cases}$ 
\item[]
\item $\vect{c}_t$ é uma soma ponderada de todas as representações de entrada: 
\begin{equation*}
\vect{c}_t = \sum_{s=1}^{n} a_{ts}\vect{h}_s
\end{equation*}
\end{itemize}
\end{frame}



\begin{frame}{Modelo de memória}
\begin{itemize}
\item $\vect{s}_1, \dots, \vect{s}_n$ sentenças de contexto
\item $\vect{q}$ pergunta
\item $\vect{a}$ resposta
\item $\{ \vect{s}_i\} \rightarrow^{\vect{A}} \{ \vect{m}_i\}$  (vetores de memôria)
\item $\vect{q} \rightarrow^{\vect{B}} \vect{u}$  (estado interno)
\item $\{ p_i\} = \{ softmax(\vect{u}^{T}\vect{m}_i)\}$  ("match" entre $\vect{m}_i$ e $\vect{u}$)
\item $\{ \vect{s}_i\} \rightarrow^{\vect{C}} \{ \vect{c}_i\}$
\item $\vect{o}= \sum_{i} p_i \vect{c}_i$
\item $\vect{\hat{a}}= softmax(\vect{W}(\vect{o} + \vect{u}))$
\end{itemize}
\end{frame}


\begin{frame}{ Podemos ter $k$ camadas e memória (hops)}
\begin{itemize}
\item $\vect{u}^{k} = \vect{u}^{k-1} + \vect{o}^{k-1}$
\item[]
\item $\{ {\vect{s}^{k}}_i\} \rightarrow^{{vect{A}^{k}}} \{ {\vect{m}^{k}}_i\}$
\item[]
\item $\{ {\vect{s}^{k}}_i\} \rightarrow^{{vect{C}^{k}}} \{ {\vect{c}^{k}}_i\}$
\item[]
\item $\{{p^{k}}_i\} = \{ softmax({\vect{u}^{k}}^{\top} {\vect{m}^{k}_{i}})\}$
\item[]
\item $\vect{o}^{k}= \sum_{i} {p^{k}}_i {\vect{c}^{k}}_i$
\item[]
\item $\vect{\hat{a}}= softmax(\vect{W}(\vect{o}^{k} + \vect{u}^{k}))$
\end{itemize}
\end{frame}

\begin{frame}{ParlAI \\ \url{https://github.com/facebookresearch/ParlAI}}

\begin{center}
\includegraphics[scale=0.84]{images/parlai.png}
\end{center}

"ParlAI (pronounced 'par-lay') is a framework for dialog AI research, implemented in Python.

Its goal is to provide researchers:

\begin{itemize}
\item a unified framework for sharing, training and testing dialog models
\item many popular datasets available all in one place, with the ability to multi-task over them
\item seamless integration of Amazon Mechanical Turk for data collection and human evaluation"
\end{itemize}

\end{frame}

\begin{frame}{Experimentos}
\begin{center}
\includegraphics[scale=0.34]{images/comparative_results_babi1.png}
\end{center}
\end{frame}

\begin{frame}{Experimentos}
\begin{center}
\includegraphics[scale=0.34]{images/comparative_results_babi2.png}
\end{center}
\end{frame}




\section{Entailment-QA}

\begin{frame}{bAbI: task 15}

\alert{Basic Deduction}

\begin{center}
\includegraphics[scale=0.28]{images/babi15.png}
\end{center}
\begin{quote} 
\centering 
$P^{1}$ are afraid of $Q^{1}$\\
$P^{2}$ are afraid of $Q^{2}$\\
$P^{3}$ are afraid of $Q^{3}$\\
$P^{4}$ are afraid of $Q^{4}$\\
$c^{1}$ is a $P^{1}$\\
$c^{2}$ is a $P^{2}$\\
$c^{3}$ is a $P^{3}$\\
$c^{4}$ is a $P^{4}$\\
What is $c^j$ afraid of? \alert{A: $Q^j$}\\
\end{quote}


\end{frame}


\begin{frame}{bAbI: task 16}

\alert{Basic Induction}

\begin{center}
\includegraphics[scale=0.28]{images/babi16.png}
\end{center}
\begin{quote} 
\centering 
$c^{1}$ is a $P^{1}$\\
$c^{1}$ is $C^{1}$\\
$c^{2}$ is a $P^{2}$\\
$c^{2}$ is $C^{2}$\\
$c^{3}$ is a $P^{3}$\\
$c^{3}$ is $C^{3}$\\
$c^{4}$ is a $P^{4}$\\
$c^{4}$ is $C^{4}$\\
$c$ is a $P^{j}$\\
What color is $c$? \alert{A: $C^j$}\\
\end{quote}

\end{frame}


\begin{frame}{SICK (Sentences Involving Compositional Knowledge) \cite{Marelli14}}

\begin{center}
\includegraphics[scale=0.25]{images/sick.png}
\end{center}

\end{frame}

\begin{frame}{Quora question pairs \cite{quora}}


Who creates bitcoins?\\
Who invented Bitcoin?\\
Are the above questions duplicate? \alert{A: no}\\


How aeroplanes fly?\\
How do airplanes fly?\\
Are the above questions duplicate? \alert{A: yes}\\



What actually is brexit?\\
What is brexit?\\
Are the above questions duplicate? \alert{A: yes}\\


hat is my ethnicity?\\
What does ethnicity mean?\\
Are the above questions duplicate? \alert{A: no}\\

\end{frame}


\begin{frame}{DialogGym}

\begin{center}
\includegraphics[scale=0.58]{images/DGred2.png}
\end{center}

\url{https://github.com/felipessalvatore/DialogGym}

\end{frame}


\begin{frame}{Um novo conjunto de tarefas}
\begin{itemize}
\item \textbf{Task 1: entailment prediction} Given two sentences $p$ and $q$ the agent is asked to detect a basic entailment relation between them, i.e., the agent should respond if $p$ implies $q$, if $p$ contradicts $q$ or if $p$ is neutral to $q$. For example, the sentences "\textit{A man is thinking}" and "\textit{There is no man thinking}" is given to the agent, he needs to detect the quantifier to spot the contradiction between these two informations.
\item \textbf{Task 2: similarity prediction} The agent is questioned to indicate how related are the meaning of two sentences, e.g., "\textit{A man is reading the email. Someone is reading the email. Are the sentences above related?}". There are only 4 possible answers: "not related", "somewhat related", "related", "strongly related". 
\item \textbf{Task 3: paraphrase prediction} The agent is asked (a yes/no question) to identify if two given questions express the same meaning using different words, e.g., "\textit{Who was Pele? Who is Pele? Are the above questions duplicate?}".
\end{itemize}
\end{frame}

\begin{frame}{Primeiros resultados}
\begin{center}
\includegraphics[scale=0.28]{images/both_semantic_tasks.png}
\end{center}
\end{frame}

\begin{frame}{Podemos melhorar os resultados para as tarefas específicas}

\begin{center}
\includegraphics[scale=0.21]{images/features_sick.png}
\end{center}

Por exemplo, em \cite{S14-2055} os autores conseguiram $84.6\%$ de accurácia no SICK.\\

\alert{Mas não queremos "tunar" um modelo para uma tarefa específica!} 

\end{frame}


\begin{frame}{Olhando as perguntas geradas pelo SICK}

The parrot is talking into the microphone\\
The parrot is speaking\\
What is the semantic relation? \alert{A: entailment}\\


There is no man cutting tomatoes\\
A man is cutting tomatoes\\
What is the semantic relation? \alert{A: contradiction}\\


Paper is being cut with scissors\\
Someone is cutting some paper with scissors\\
What is the semantic relation? \alert{A: entailment}\\


An elder man is sitting on a bench and is angry\\
An elderly man is sitting on a bench\\
What is the semantic relation? \alert{A: entailment}\\
\end{frame}


\begin{frame}{Entailment-QA}

\begin{enumerate}
\item \textbf{Boolean Connectives}
\item[]
\item \textbf{First-Order Quantifiers}
\item[]
\item \textbf{Synonymy}
\item[]
\item \textbf{Antinomy}
\item[]
\item \textbf{Hypernymy}
\item[]
\item \textbf{Active/Passive voice}
\end{enumerate}
\end{frame}

\begin{frame}{Entailment-QA: task 1}
\begin{itemize}
\item \alert{Entailment} ($s_1$ implies $s_2$)
\begin{itemize}
\item $\underbrace{P^{1}a^1 \land \dots \land P^{n}a^n}_{s_1}, \underbrace{P^{j}a^j}_{s_2}$ 
\item $\underbrace{P^{j}a^j}_{s_1}, \underbrace{P^{1}a^1 \lor \dots \lor P^{n}a^n}_{s_2}$
\item $\underbrace{Pa}_{s_1}, \underbrace{\lnot \lnot Pa}_{s_2}$
\end{itemize}

\vspace{0.4cm}
\item \alert{Not entailment} ($s_1$ does not imply $s_2$)
\begin{itemize}
\item $\underbrace{P^{j}a^j}_{s_1}, \underbrace{P^{1}a^1 \land \dots \land P^{n}a^n}_{s_2}$ 
\item $\underbrace{P^{1}a^1 \lor \dots \lor P^{n}a^n}_{s_1}, \underbrace{P^{j}a^j}_{s_2}$
\item $\underbrace{Pa}_{s_1}, \underbrace{\lnot Pa}_{s_2}$
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Entailment-QA: task 1}
\begin{itemize} 
\item[] Ashley is fit
\item[] Ashley is not fit
\item[] The first sentence implies the second sentence? \alert{A: no}
\end{itemize}

\vspace{0.3cm}


\begin{itemize} 
\item[]Avery is nice and Avery is obedient
\item[]Avery is nice
\item[]The first sentence implies the second sentence? \alert{A: yes}
\end{itemize}

\vspace{0.3cm}

\begin{itemize} 
\item[]Elbert is handsome or Elbert is long
\item[]Elbert is handsome
\item[]The first sentence implies the second sentence? \alert{A: no}
\end{itemize}
\end{frame}


\begin{frame}{Entailment-QA: task 2}

\begin{itemize}
\item \alert{Entailment}
\begin{itemize}
\item $\forall x Px, Pa$ 
\item $Pa, \exists x Px$ 
\end{itemize}
\item \alert{Contradiction}
\begin{itemize}
\item $\forall x Px, \lnot Pa$ 
\item $\forall x Px, \exists x \lnot Px$ 
\end{itemize}
\item \alert{Neutral}
\begin{itemize}
\item $Pa,Qa$ 
\item $\forall x Px, \lnot Qa$ 
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Entailment-QA: task 2}

\begin{itemize} 
\item[] Every person is lively
\item[] Belden is lively
\item[] What is the semantic relation? \alert{A: entailment}
\end{itemize}

\begin{itemize} 
\item[] Every person is short
\item[] There is one person that is not short
\item[] What is the semantic relation?  \alert{A: contradiction}
\end{itemize}

\begin{itemize} 
\item[] Every person is beautiful
\item[] Abilene is not blue
\item[] What is the semantic relation? \alert{A: neutral}
\end{itemize}
\end{frame}



\begin{frame}{Resultados até agora}
\begin{center}
\includegraphics[scale=0.42]{images/comparative_results.png}
\end{center}
\end{frame}



\begin{frame}{Resultados até agora}
\begin{center}
\includegraphics[scale=0.28]{images/training_acc_EntailQA2_mem.png}
\end{center}
\end{frame}


\begin{frame}{Resultados até agora}
\begin{center}
\includegraphics[scale=0.42]{images/cm_mem_EntailQA2.png}
\end{center}
\end{frame}


\begin{frame}{Próximos passos}
\begin{itemize}

\item Terminar as tarefas

\item Melhor o treinamento com os modelos atuais

\item Explorar novos modelos
\end{itemize}


\end{frame}

\begin{frame}[allowframebreaks]{Referências}

  \bibliography{my_references}
  \bibliographystyle{abbrv}

\end{frame}

\end{document}




\end{document}