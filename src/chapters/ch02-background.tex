\chapter{Background}
\label{ch:02-background}

Different deep learning architectures are used in NLP: \textbf{convolutional architectures} have a good performance in tasks were it is required to find a linguistic indicator regardless of its position (e.g., document classification, short-text categorization, sentiment classification, etc); high quality word embeddings can be achieved with models that are a kind of \textbf{feedforward neural network} \cite{Mikolov23}. But for a variety of works in natural language we want to capture regularities and similarities in a text structure. That is way \textbf{recurrent} and \textbf{recursive} models have been widely used in the field. Here we are focused on generative models and since recurrent models have been producing very strong results for language modeling \cite{goldberg15}, we will concentrate on them.

\section{RNN}
\label{sec:RNN}


Recurrent Neural Network is a family of neural network specialized in sequential data $\vect{x}_1, \dots, \vect{x}_\tau$. As a neural network, a RNN is a parametrized function that we use to approximate one hidden function from the data. As before we can take the simplest RNN as a neural network with only one hidden layer. But now, what make RNNs unique is a recurrent definition of one of its hidden layer:

\begin{equation}
\vect{h}^{(t)} = g(\vect{h}^{(t-1)}, \vect{x}^{(t)}; \vect{\theta})
\end{equation}

$\vect{h}^{(t)}$ is called \textit{state}, \textit{hidden state}, or \textbf{cell}.

Is costumerely to represent a RNN as a ciclic graph \ref{RNNSimplified}

\input{tikz/RNNSimplified}

\par This recurrent equation can be unfolded for a finite number of steps $\tau$. For example, when $\tau =3$:
\vspace{0.2cm}

\begin{align}
\vect{h}^{(3)}& = g(\vect{h}^{(2)}, \vect{x}^{(3)}; \vect{\theta})\\
 & = g(g(\vect{h}^{(1)}, \vect{x}^{(2)}; \vect{\theta}), \vect{x}^{(3)}; \vect{\theta})\\
 & = g(g(g(\vect{h}^{(0)}, \vect{x}^{(1)}; \vect{\theta}), \vect{x}^{(2)}; \vect{\theta}), \vect{x}^{(3)}; \vect{\theta})\\
\end{align}



Hence for any finite step $\tau$ we can describe the model as a DAG \ref{RNNSimplifiedUnfolded}.

\input{tikz/RNNSimplifiedUnfolded}

Using a concret example consider the following model define by the equations:

\begin{equation}
f(\vect{x}^{(t)}, \vect{h}^{(t-1)}; \vect{V}, \vect{W}, \vect{U}, \vect{c}, \vect{b}) = \vect{\hat{y}}^{(t)}
\end{equation}
 \vspace{0.2cm}
\begin{equation}
\vect{\hat{y}}^{(t)} = softmax(\vect{V} \vect{h}^{(t)} + \vect{c})
\end{equation}
\vspace{0.2cm}
 \begin{equation}
\vect{h}^{(t)} = g(\vect{h}^{(t-1)}, \vect{x}^{(t)}; \vect{W},\vect{U}, \vect{b})
\end{equation}
\vspace{0.2cm}
\begin{equation}
\vect{h}^{(t)} = \sigma(\vect{W} \vect{h}^{(t-1)} + \vect{U} \vect{x}^{(t)} + \vect{b})
\end{equation}

Using the graphical representation the model can be view as:

\input{tikz/RNNGraphExpanded} 

!!! explicar como funciona o treinamento !!!

An RNN with a loss function:

\input{tikz/rnn-time-unfolding}


\section{GRU}
\label{sec:GRU}


To capture long-term dependencies on a RNN  the authors of the paper \cite{ChungGCB14}  proposed a new architecture called \textbf{gated recurrent unit} (\textbf{GRU}). This model was constructed to make each hidden state  $\vect{h}^{(t)}$ to adaptively capture dependencies of different time steps. It work as follows, at each step $t$ one candidate for hidden state is formed:

\begin{equation}
\vect{\widetilde{h}}^{(t)} = tahn(\vect{W} (\vect{h}^{(t-1)} \odot  \vect{r}^{(t)}) + \vect{U} \vect{x}^{(t)} + \vect{b})
\end{equation}


where $\vect{r}^{(t)}$ is a vector with values in $[0, 1]$ called a \textbf{reset gate}, i.e.,  a vector that at each entry outputs the probability of reseting the  corresponding entry in the previous hidden state $\vect{h}^{(t-1)}$. Together with $\vect{r}^{(t)}$ we define an \textbf{update gate}, $\vect{u}^{(t)}$. It is also a vector with values in $[0, 1]$. Intuitively we can say that this vector decides how much on each dimension we will use the candidate update. Both $\vect{r}^{(t)}$ and $\vect{u}^{(t)}$ are defined by $\vect{h}^{(t-1)}$ and $\vect{x}^{(t)}$; they also have specific parameters:

\begin{equation}
\vect{r}^{(t)} = \sigma(\vect{W}_{r} \vect{h}^{(t-1)} + \vect{U}_{r} \vect{x}^{(t)} + \vect{b}_{r})
\end{equation}


\begin{equation}
\vect{u}^{(t)} = \sigma(\vect{W}_{u} \vect{h}^{(t-1)} + \vect{U}_{u} \vect{x}^{(t)} + \vect{b}_{u})
\end{equation}

At the end the new hidden state $\vect{h}^{(t)}$ is defined by the recurrence:

\begin{equation}
\vect{h}^{(t)} = \vect{u}^{(t)} \odot \vect{\widetilde{h}}^{(t)} + (1 - \vect{u}^{(t)}) \odot \vect{h}^{(t-1)} 
\end{equation}

Note that the new hidden state combines the candidate hidden state $\vect{\widetilde{h}}^{(t)}$ with the past hidden state $\vect{h}^{(t-1)}$ using both $\vect{r}^{(t)}$ and $\vect{u}^{(t)}$ to adaptively copy and forget information.

\par It can appear more complex, but we can view the GRU model just as a refinement of the standard RNN with a new computation for the hidden state. Let $\vect{\theta} = [ \vect{W},\vect{U},\vect{b} ]$, $\vect{\theta}_{u} = [ \vect{W}_{u},\vect{U}_{u},\vect{b}_{u} ]$  and $\vect{\theta}_{r} = [ \vect{W}_{r},\vect{U}_{r},\vect{b}_{r} ]$; and $\text{aff}(\vect{\theta})$ be the following operation:

\begin{equation}
\text{aff}(\vect{\theta}) = \vect{W} \vect{h} + \vect{U} \vect{x} + \vect{b}
\end{equation}

With similar definitions for $\text{aff}(\vect{\theta}_{u})$ and $\text{aff}(\vect{\theta}_{r})$. Figure \ref{GRU} shows the hidden state of the GRU model for time step $t$. If compared with Figure \ref{RNNGraphExpanded} we can see that the basic structure is the same, just the way of computing the hidden state $\vect{h}^{(t)}$ has changed.

 \input{tikz/GRU_cell}

\section{LSTM}
\label{sec:LSTM}


\textit{Long short-term memory} (LSTM) is one of the most applied versions of the RNN family of models. Historically it was developed before the GRU model, but conceptually we can think in the RNN as an expansion of the model presented in the last session. Because of notation differences they can look different. LSTM is based also with parametrized gates; in this case three: the \textbf{forget gate}, $\vect{f}^{(t)}$, the \textbf{input gate}, $\vect{i}^{(t)}$, and the \textbf{output gate}, $\vect{o}^{(t)}$. There gates are defined only by $\vect{h}^{(t-1)}$ and $\vect{x}^{(t)}$ with specific parameters:

\begin{equation}
\vect{f}^{(t)} = \sigma(\vect{W}_{f} \vect{h}^{(t-1)} + \vect{U}_{f} \vect{x}^{(t)} + \vect{b}_{f})
\end{equation}

\begin{equation}
\vect{i}^{(t)} = \sigma(\vect{W}_{i} \vect{h}^{(t-1)} + \vect{U}_{i} \vect{x}^{(t)} + \vect{b}_{i})
\end{equation}

\begin{equation}
\vect{o}^{(t)} = \sigma(\vect{W}_{o} \vect{h}^{(t-1)} + \vect{U}_{o} \vect{x}^{(t)} + \vect{b}_{o})
\end{equation}

Intuitively $\vect{f}^{(t)}$ should control how much informative be discarded, $\vect{i}^{(t)}$ controls how much information will be updated, and $\vect{o}^{(t)}$ controls how munch each component should be outputted. A candidate cell, $\tilde{\vect{c}}^{(t)}$ is formed:

\begin{equation}
\tilde{\vect{c}}^{(t)} = tahn(\vect{W} \vect{h}^{(t-1)} + \vect{U} \vect{x}^{(t)} + \vect{b})
\end{equation}

and a new cell $\tilde{\vect{c}}^{(t)}$ is formed by forgetting some information of the previous cell $\tilde{\vect{c}}^{(t-1)}$ and by adding new values from $\tilde{\vect{c}}^{(t)}$ (scaled by the input gate)

\begin{equation}
\vect{c}^{(t)} = \vect{f}^{(t)} \otimes \vect{c}^{(t-1)} + \vect{i}^{(t)}\otimes \tilde{\vect{c}}^{(t)}
\end{equation}

The new hidden state, $\vect{h}^{(t)}$, is formed by filtering $\vect{c}^{(t)}$:

\begin{equation}
\vect{h}^{(t)} = \vect{o}^{(t)} \otimes tanh(\vect{c}^{(t)})
\end{equation}


\input{tikz/LSTM_cell}


\section{Language model}

We call \textit{language model} a probability distribution over sequences of tokens in a natural language.

\[
P(x_1,x_2,x_3,x_4) = p
\]

This model is used for different nlp tasks such as speech recognition, machine translation, text auto-completion, spell correction, question answering, summarization and many others.

The classical approuch to a languange model was to use the chain rule and a markovian assumptiom, i.e., for a specific $n$ we assume that:

\begin{equation}
P(x_1, \dots, x_T) = \prod_{t=1}^{T} P(x_t \vert x_1, \dots, x_{t-1}) = \prod_{t=1}^{T} P(x_{t} \vert x_{t - (n+1)}, \dots, x_{t-1})
\end{equation} 


This gave raise to models based on $n$-gram statistics. The choice of $n$ yields different models; for example 
\textit{Unigram} language model ($n=1$): 
\begin{equation}
P_{uni}(x_1, x_2, x_3, x_4) = P(x_1)P(x_2)P(x_3)P(x_4)
\end{equation}

where $P(x_i) = count(x_i)$.\\

\textit{Bigram} language model ($n=2$): 
\begin{equation}
P_{bi}(x_1,x_2,x_3,x_4) = P(x_1)P(x_2\vert x_1)P(x_3\vert x_2)P(x_4\vert x_3)
\end{equation} 
where
\[
P(x_i\vert x_j) = \frac{count(x_i, x_j)}{count(x_j)}
\]


Higher $n$-grams yields better performance. But at the same time higher $n$-grams requires a lot of memory\cite{Heafield}.

Since \cite{Mikolov11} the approach has change, instead of using one approach that is specific for the language domain, we can use a general model for sequential data prediction: a RNN.

So, our learning task is to estimate the probability distribution 

\[
P(x_{n} = \text{word}_{j^{*}} | x_{1}, \dots ,x_{n-1})
\]

for any $(n-1)$-sequence of words $x_{1}, \dots ,x_{n-1}$.

We start with a corpus $C$ with $T$ tokens and a vocabulary $\Vocab$.\\\

Example: \textbf{Make Some Noise} by the Beastie Boys.\\

\begin{quote}
Yes, here we go again, give you more, nothing lesser\\
Back on the mic is the anti-depressor\\
Ad-Rock, the pressure, yes, we need this\\
The best is yet to come, and yes, believe this\\
... \\
\end{quote}

\begin{itemize}
\item $T = 378$
\item $|\Vocab| = 186$
\end{itemize}


The dataset is a collection of pairs $(\vect{x},\vect{y})$ where $\vect{x}$ is one word and $\vect{y}$ is the immediately next word. For example:
\begin{itemize}
\item [] $(\vect{x}^{(1)}, \vect{y}^{(1)}) =$ (Yes, here).
\item [] $(\vect{x}^{(2)}, \vect{y}^{(2)}) =$ (here, we)
\item [] $(\vect{x}^{(3)}, \vect{y}^{(3)}) =$ (we, go)
\item [] $(\vect{x}^{(4)}, \vect{y}^{(4)}) =$ (go, again)
\item [] $(\vect{x}^{(5)}, \vect{y}^{(5)}) =$ (again, give)
\item [] $(\vect{x}^{(6)}, \vect{y}^{(6)}) =$ (give, you)
\item [] $(\vect{x}^{(7)}, \vect{y}^{(7)}) =$ (you, more)
\item [] $\dots$
\end{itemize}

Notation

\begin{itemize}
\item $\vect{E} \in \mathbb{R}^{d,|\Vocab|}$ is the matrix of word embeddings.
\vspace{0.3cm}
\item $\vect{x}^{(t)} \in \mathbb{R}^{|\Vocab|}$ is one-hot word vector at time step $t$.
\vspace{0.3cm}
\item $\vect{y}^{(t)} \in \mathbb{R}^{|\Vocab|}$ is the ground truth at time step $t$ (also an one-hot word vector).
\end{itemize}

The language model is similar as the RNN described above. It is defined by the following equations:

\begin{equation}
\vect{e}^{(t)} = \vect{E}\vect{x}^{(t)}
\end{equation}
\vspace{0.2cm}
 \begin{equation}
\vect{h}^{(t)} = \sigma(\vect{W}\vect{h}^{(t-1)}+ \vect{U}\vect{e}^{(t)}+ \vect{b})
\end{equation}
\vspace{0.2cm}
\begin{equation}
\vect{\hat{y}}^{(t)} = softmax(\vect{V}\vect{h}^{(t)} + \vect{c})
\end{equation}

\input{tikz/LanguageModelExpanded}


At each time $t$ the point-wise loss is:

\vspace{0.2cm}

\begin{align}
L^{(t)} &= CE(\vect{y}^{(t)},\vect{\hat{y}}^{(t)})\\
    &= - \log(\vect{\hat{y}}_{j^{*}})\\
        &= - \log P(x^{(t+1)} = \text{word}_{j^{*}}|x^{(1)}, \dots, x^{(t)})
\end{align}

The loss $L$ is the mean of all the point-wise losses
\begin{equation}
L=\frac{1}{T}\sum_{t=1}^{T}L^{(t)}
\end{equation}


Evaluating a language model. We can evaluate a  language model using a \textit{extrinsic evaluation}: How our model perform in a NLP task such as text auto-completion. Or a \textit{intrinsic evaluation}: Perplexity (PP) can be thought as the weighted average branching factor of a language.


Given $C= x_1, x_2, \dots, x_T$, we define the perplexity of $C$ as:

\begin{align}
PP(C) &= P(x_1, x_2, \dots, x_T)^{-\frac{1}{T}}\\
    & \\
      &= \sqrt[T]{\frac{1}{P(x_1, x_2, \dots, x_T)}}\\
      & \\
      &= \sqrt[T]{\prod_{i=1}^{T}\frac{1}{P(x_i \vert x_1,\dots, x_{i-1})}}
\end{align}

we can relate Loss and Perplexity:

Since
\begin{align}
L^{(t)} & = - \log P(x^{(t+1)} |x^{(1)}, \dots, x^{(t)})\\
& =  \log(\frac{1}{P(x^{(t+1)}|x^{(1)}, \dots, x^{(t)})})\\
\end{align}
We have that:

\begin{align}
        L &=\frac{1}{T} \sum_{t=1}^{T} L^{(t)}\\
          &= \log\left( \sqrt[T]{\prod_{i=1}^{T}\frac{1}{P(x_i \vert x_1,\dots, x_{i-1})}} \right)\\
          &= \log(PP(C))
\end{align}

So another definition of perplexity is

\begin{equation}
2^{L} = PP(C)
\end{equation}





\section{Seq2seq}
\label{sec:Seq2seq}

fsdfdsfdsf



\section{Atention}
\label{sec:Atention}

fksdhfjsdgjf

