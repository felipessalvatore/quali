\chapter{Background}
\label{ch:02-background}

We can make human language manageable to computers by using a sort of different methods and techniques. This is the bread and butter of any NLP researcher. Here, following today's NLP community, we will focus only in the techniques derived from the machine learning field. In this chapter, we will present some abstract machine learning models follow by some applications in NLP. 

\section{Machine Learning}

\textit{Machine learning} is the branch of computer science that deals with programs that can improve with experience (i., learn). Machine learning is divided into three main subareas: \textit{supervised learning}, \textit{reinforcement learning} and \textit{unsupervised learning}.

Here we will work with supervised learning only. In this setting, we assume that the process of interest is defined by an unknown function $g:X\rightarrow Y$. We try to approximate $g$ by changing the parameters of a function $f$ through an optimization process. The optimization is done by defining an \textit{error function} that evaluates how well $f$ approximates $g$ in the part of $g$ that we have access: the training data $D = \{(\vect{x}_1, \vect{y}_1), \dots ,(\vect{x}_N, \vect{y}_N)\}$ (where $g(\vect{x}_i)=\vect{y}_i$).


$f$ can be a function from any family of models. One family that is having a lot of success for language tasks is the \textit{neural network} family.

\section{Neural Networks}

A neural network is a non-linear function $f(\vect{x}; \theta)$. It is defined by a collection of parameters $\vect{\theta}$ and a collection of non-linear transformations. It is usual to represent $f$ as a compositions of functions:

\begin{align}
f(\vect{x}; \theta) &= f^{(2)}(f^{(1)}(\vect{x}; \vect{W}_1, \vect{b}_1); \vect{W}_2, \vect{b}_2)\\
&= softmax(\vect{W}_2 (\sigma(\vect{W}_1\vect{x} + \vect{b}_1)) + \vect{b}_2)
\end{align}


The output of these intermediary functions are referred as \textit{layers}. So in the example above, $\vect{x}$ (the output of the identity function) is the \textit{input layer}, $f^{(1)}(\vect{x}; \vect{W}_1, \vect{b}_1)$ is the \textit{hidden layer} and $f^{(2)}(f^{(1)}(\vect{x}; \vect{W}_1, \vect{b}_1); \vect{W}_2, \vect{b}_2)$ is the \textit{output layer}. Since each layer is a vector, we normally speak about the \textit{dimension} of a layer. For historical reasons we also say that each entry on a layer is a \textit{node} or a \textit{neuron}.  Models with a large number of hidden layers are called \textit{deep models}, for this reason the name \textit{deep learning} is used.  

\par A neural network is a function approximator: it can approximate any Borel measurable function from one finite dimensional space to another with any desired nonzero amount of error. This theoretical result is know as the \textit{universal approximation theorem}\cite{Cybenko}. Without entering in the theoretical concepts, it suffice to note that the family of Borel mensurable functions include all continuous functions on a closed and bounded subset of $\mathbb{R}^n$.



Different deep learning architectures are used in NLP: \textit{convolutional architectures} have a good performance in tasks were it is required to find a linguistic indicator regardless of its position (e.g., document classification, short-text categorization, sentiment classification, etc); high quality word embeddings can be achieved with models that are a kind of \textit{feedforward neural network} \cite{Mikolov23}. But for a variety of works in natural language we want to capture regularities and similarities in a text structure. That is way \textit{recurrent} and \textit{recursive} models have been widely used in the field. Here we are focused on generative models and since recurrent models have been producing very strong results for language modeling \cite{goldberg15}, we will concentrate on them.

\section{Recurrent Neural Network}
\label{sec:RNN}


\textit{Recurrent neural network} (RNN) is a family of neural network specialized in sequential data $\vect{x}_1, \dots, \vect{x}_\tau$. As a neural network, a RNN is a parametrized function that we use to approximate one hidden function from the data. As before we can take the simplest RNN as a neural network with only one hidden layer. But now, what make RNNs unique is a recurrent definition of one of its hidden layer:

\begin{equation}
\vect{h}^{(t)} = g(\vect{h}^{(t-1)}, \vect{x}^{(t)}; \vect{\theta})
\end{equation}

$\vect{h}^{(t)}$ is called \textit{state}, \textit{hidden state}, or \textit{cell}.


\par This recurrent equation can be unfolded for a finite number of steps $\tau$. For example, when $\tau =3$:
\vspace{0.2cm}

\begin{align}
\vect{h}^{(3)}& = g(\vect{h}^{(2)}, \vect{x}^{(3)}; \vect{\theta})\\
 & = g(g(\vect{h}^{(1)}, \vect{x}^{(2)}; \vect{\theta}), \vect{x}^{(3)}; \vect{\theta})\\
 & = g(g(g(\vect{h}^{(0)}, \vect{x}^{(1)}; \vect{\theta}), \vect{x}^{(2)}; \vect{\theta}), \vect{x}^{(3)}; \vect{\theta})\\
\end{align}

Using a concret example consider the following classification model define by the equations:

\begin{equation}
f(\vect{x}^{(t)}, \vect{h}^{(t-1)}; \vect{V}, \vect{W}, \vect{U}, \vect{c}, \vect{b}) = \vect{\hat{y}}^{(t)}
\end{equation}
 \vspace{0.2cm}
\begin{equation}
\vect{\hat{y}}^{(t)} = softmax(\vect{V} \vect{h}^{(t)} + \vect{c})
\end{equation}
\vspace{0.2cm}
 \begin{equation}
\vect{h}^{(t)} = g(\vect{h}^{(t-1)}, \vect{x}^{(t)}; \vect{W},\vect{U}, \vect{b})
\end{equation}
\vspace{0.2cm}
\begin{equation}
\vect{h}^{(t)} = \sigma(\vect{W} \vect{h}^{(t-1)} + \vect{U} \vect{x}^{(t)} + \vect{b})
\end{equation}

This kind of model can create an output $\vect{\hat{y}}^{(t)}$ at each time $t$, or the model can produce a single output $\vect{\hat{y}}$ after processing an entire input sequence. This choice depends on the learning problem that is being modeled.

With the model's prediction at hand, we can use a loss function (like cross entropy for the classification problem) and apply the back-propagation algorithm to optimize the model. These models look complex, but it quite straightforward to compute the gradients \cite[p.~374]{DeepLearningbook}.

Although this kind of deep learning model is very useful, it presents a severe flaw. When computing the gradients there is a lot of repeated matrix multiplication using the recurrent weight matrix (in the example above, the matrix $\vect{W}$). Depending on some configurations of this matrix \textit{the gradients may vanish or explode exponentially with respect to the number of time steps}.

Thus, handing long-term dependencies became a problem when using RNNs. Different solutions were proposed, the most effective results came from some modifications of this model. We will present the two most famous modifications: the gated recurrent unit and the long short-term memory. 

\section{Gated Recurrent Unit}
\label{sec:GRU}

To capture long-term dependencies on a RNN  the authors of the paper \cite{ChungGCB14}  proposed a new architecture called \textit{gated recurrent unit} (GRU). This model was constructed to make each hidden state  $\vect{h}^{(t)}$ to adaptively capture dependencies of different time steps. It work as follows, at each step $t$ one candidate for hidden state is formed:

\begin{equation}
\vect{\widetilde{h}}^{(t)} = tahn(\vect{W} (\vect{h}^{(t-1)} \odot  \vect{r}^{(t)}) + \vect{U} \vect{x}^{(t)} + \vect{b})
\end{equation}


where $\vect{r}^{(t)}$ is a vector with values in $[0, 1]$ called a \textit{reset gate}, i.e.,  a vector that at each entry outputs the probability of reseting the  corresponding entry in the previous hidden state $\vect{h}^{(t-1)}$. Together with $\vect{r}^{(t)}$ we define an \textit{update gate}, $\vect{u}^{(t)}$. It is also a vector with values in $[0, 1]$. Intuitively we can say that this vector decides how much on each dimension we will use the candidate update. Both $\vect{r}^{(t)}$ and $\vect{u}^{(t)}$ are defined by $\vect{h}^{(t-1)}$ and $\vect{x}^{(t)}$; they also have specific parameters:

\begin{equation}
\vect{r}^{(t)} = \sigma(\vect{W}_{r} \vect{h}^{(t-1)} + \vect{U}_{r} \vect{x}^{(t)} + \vect{b}_{r})
\end{equation}


\begin{equation}
\vect{u}^{(t)} = \sigma(\vect{W}_{u} \vect{h}^{(t-1)} + \vect{U}_{u} \vect{x}^{(t)} + \vect{b}_{u})
\end{equation}

At the end the new hidden state $\vect{h}^{(t)}$ is defined by the recurrence:

\begin{equation}
\vect{h}^{(t)} = \vect{u}^{(t)} \odot \vect{\widetilde{h}}^{(t)} + (1 - \vect{u}^{(t)}) \odot \vect{h}^{(t-1)} 
\end{equation}

Note that the new hidden state combines the candidate hidden state $\vect{\widetilde{h}}^{(t)}$ with the past hidden state $\vect{h}^{(t-1)}$ using both $\vect{r}^{(t)}$ and $\vect{u}^{(t)}$ to adaptively copy and forget information.

\section{Long Short-Term Memory}
\label{sec:LSTM}


\textit{Long short-term memory} (LSTM) is one of the most applied versions of the RNN family of models. Historically it was developed before the GRU model, but conceptually we can think in the RNN as an expansion of the model presented in the last session. Because of notation differences they can look different. LSTM is also based on parametrized gates; in this case three: the \textit{forget gate}, $\vect{f}^{(t)}$, the \textit{input gate}, $\vect{i}^{(t)}$, and the \textit{output gate}, $\vect{o}^{(t)}$. The gates are defined only by $\vect{h}^{(t-1)}$ and $\vect{x}^{(t)}$ with specific parameters:

\begin{equation}
\vect{f}^{(t)} = \sigma(\vect{W}_{f} \vect{h}^{(t-1)} + \vect{U}_{f} \vect{x}^{(t)} + \vect{b}_{f})
\end{equation}

\begin{equation}
\vect{i}^{(t)} = \sigma(\vect{W}_{i} \vect{h}^{(t-1)} + \vect{U}_{i} \vect{x}^{(t)} + \vect{b}_{i})
\end{equation}

\begin{equation}
\vect{o}^{(t)} = \sigma(\vect{W}_{o} \vect{h}^{(t-1)} + \vect{U}_{o} \vect{x}^{(t)} + \vect{b}_{o})
\end{equation}

Intuitively $\vect{f}^{(t)}$ should control how much informative will be discarded, $\vect{i}^{(t)}$ controls how much information will be updated, and $\vect{o}^{(t)}$ controls how munch each component should be outputted. A candidate cell, $\tilde{\vect{c}}^{(t)}$ is formed:

\begin{equation}
\tilde{\vect{c}}^{(t)} = tahn(\vect{W} \vect{h}^{(t-1)} + \vect{U} \vect{x}^{(t)} + \vect{b})
\end{equation}

and a new cell $\tilde{\vect{c}}^{(t)}$ is formed by forgetting some information of the previous cell $\tilde{\vect{c}}^{(t-1)}$ and by adding new values from $\tilde{\vect{c}}^{(t)}$ (scaled by the input gate)

\begin{equation}
\vect{c}^{(t)} = \vect{f}^{(t)} \otimes \vect{c}^{(t-1)} + \vect{i}^{(t)}\otimes \tilde{\vect{c}}^{(t)}
\end{equation}

The new hidden state, $\vect{h}^{(t)}$, is formed by filtering $\vect{c}^{(t)}$:

\begin{equation}
\vect{h}^{(t)} = \vect{o}^{(t)} \otimes tanh(\vect{c}^{(t)})
\end{equation}


Until now we have presented general deep learning theory, now we will focus on the specificities of these models applied to natural language problems.


\section{Language model}

We call \textit{language model} a probability distribution over sequences of tokens in a natural language.

\[
P(x_1,x_2,x_3,x_4) = p
\]

This model is used for different NLP tasks such as speech recognition, machine translation, text auto-completion, spell correction, question answering, summarization and many others.

The classical approuch to a languange model was to use the chain rule and a markovian assumptiom, i.e., for a specific $n$ we assume that:

\begin{equation}
P(x_1, \dots, x_T) = \prod_{t=1}^{T} P(x_t \vert x_1, \dots, x_{t-1}) = \prod_{t=1}^{T} P(x_{t} \vert x_{t - (n+1)}, \dots, x_{t-1})
\end{equation} 


This gave raise to models based on $n$-gram statistics. The choice of $n$ yields different models; for example 
\textit{Unigram} language model ($n=1$): 
\begin{equation}
P_{uni}(x_1, x_2, x_3, x_4) = P(x_1)P(x_2)P(x_3)P(x_4)
\end{equation}

where $P(x_i) = count(x_i)$.\\

\textit{Bigram} language model ($n=2$): 
\begin{equation}
P_{bi}(x_1,x_2,x_3,x_4) = P(x_1)P(x_2\vert x_1)P(x_3\vert x_2)P(x_4\vert x_3)
\end{equation} 
where

\begin{equation}
P(x_i\vert x_j) = \frac{count(x_i, x_j)}{count(x_j)}
\end{equation} 


Higher $n$-grams yields better performance. But at the same time higher $n$-grams requires a lot of memory\cite{Heafield}.

Since \cite{Mikolov11} the approach has change, instead of using one approach that is specific for the language domain, we can use a general model for sequential data prediction: a RNN.

So, our learning task is to estimate the probability distribution 

\[
P(x_{n} = \text{word}_{j^{*}} | x_{1}, \dots ,x_{n-1})
\]

for any $(n-1)$-sequence of words $x_{1}, \dots ,x_{n-1}$.

We start with a corpus $C$ with $T$ tokens and a vocabulary $\Vocab$.\\\

Example: \textit{Make Some Noise} by the Beastie Boys.\\

\begin{quote}
Yes, here we go again, give you more, nothing lesser\\
Back on the mic is the anti-depressor\\
Ad-Rock, the pressure, yes, we need this\\
The best is yet to come, and yes, believe this\\
... \\
\end{quote}

\begin{itemize}
\item $T = 378$
\item $|\Vocab| = 186$
\end{itemize}


The dataset is a collection of pairs $(\vect{x},\vect{y})$ where $\vect{x}$ is one word and $\vect{y}$ is the immediately next word. For example:
\begin{itemize}
\item [] $(\vect{x}^{(1)}, \vect{y}^{(1)}) =$ (Yes, here).
\item [] $(\vect{x}^{(2)}, \vect{y}^{(2)}) =$ (here, we)
\item [] $(\vect{x}^{(3)}, \vect{y}^{(3)}) =$ (we, go)
\item [] $(\vect{x}^{(4)}, \vect{y}^{(4)}) =$ (go, again)
\item [] $(\vect{x}^{(5)}, \vect{y}^{(5)}) =$ (again, give)
\item [] $(\vect{x}^{(6)}, \vect{y}^{(6)}) =$ (give, you)
\item [] $(\vect{x}^{(7)}, \vect{y}^{(7)}) =$ (you, more)
\item [] $\dots$
\end{itemize}

Notation

\begin{itemize}
\item $\vect{E} \in \mathbb{R}^{d,|\Vocab|}$ is the matrix of word embeddings.
\vspace{0.3cm}
\item $\vect{x}^{(t)} \in \mathbb{R}^{|\Vocab|}$ is one-hot word vector at time step $t$.
\vspace{0.3cm}
\item $\vect{y}^{(t)} \in \mathbb{R}^{|\Vocab|}$ is the ground truth at time step $t$ (also an one-hot word vector).
\end{itemize}

The language model is similar as the RNN described above. It is defined by the following equations:

\begin{equation}
\vect{e}^{(t)} = \vect{E}\vect{x}^{(t)}
\end{equation}
\vspace{0.2cm}
 \begin{equation}
\vect{h}^{(t)} = \sigma(\vect{W}\vect{h}^{(t-1)}+ \vect{U}\vect{e}^{(t)}+ \vect{b})
\end{equation}
\vspace{0.2cm}
\begin{equation}
\vect{\hat{y}}^{(t)} = softmax(\vect{V}\vect{h}^{(t)} + \vect{c})
\end{equation}



At each time $t$ the point-wise loss is:

\vspace{0.2cm}

\begin{align}
L^{(t)} &= CE(\vect{y}^{(t)},\vect{\hat{y}}^{(t)})\\
    &= - \log(\vect{\hat{y}}_{j^{*}})\\
        &= - \log P(x^{(t+1)} = \text{word}_{j^{*}}|x^{(1)}, \dots, x^{(t)})
\end{align}

The loss $L$ is the mean of all the point-wise losses
\begin{equation}
L=\frac{1}{T}\sum_{t=1}^{T}L^{(t)}
\end{equation}


Evaluating a language model. We can evaluate a  language model using a \textit{extrinsic evaluation}: How our model perform in a NLP task such as text auto-completion. Or a \textit{intrinsic evaluation}: Perplexity (PP) can be thought as the weighted average branching factor of a language.


Given $C= x_1, x_2, \dots, x_T$, we define the perplexity of $C$ as:

\begin{align}
PP(C) &= P(x_1, x_2, \dots, x_T)^{-\frac{1}{T}}\\
    & \\
      &= \sqrt[T]{\frac{1}{P(x_1, x_2, \dots, x_T)}}\\
      & \\
      &= \sqrt[T]{\prod_{i=1}^{T}\frac{1}{P(x_i \vert x_1,\dots, x_{i-1})}}
\end{align}

we can relate Loss and Perplexity:

Since
\begin{align}
L^{(t)} & = - \log P(x^{(t+1)} |x^{(1)}, \dots, x^{(t)})\\
& =  \log(\frac{1}{P(x^{(t+1)}|x^{(1)}, \dots, x^{(t)})})\\
\end{align}
We have that:

\begin{align}
        L &=\frac{1}{T} \sum_{t=1}^{T} L^{(t)}\\
          &= \log\left( \sqrt[T]{\prod_{i=1}^{T}\frac{1}{P(x_i \vert x_1,\dots, x_{i-1})}} \right)\\
          &= \log(PP(C))
\end{align}

So another definition of perplexity is

\begin{equation}
2^{L} = PP(C)
\end{equation}





\section{Seq2seq}
\label{sec:Seq2seq}

fsdfdsfdsf



\section{Atention}
\label{sec:Atention}

fksdhfjsdgjf

