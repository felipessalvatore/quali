\chapter{Introduction}
\label{ch:01-introduction}

One of the main goals in NLP is to build agents capable of \textit{understanding language and reasoning}, i.e., agents capable of carrying a conversation as if they were human. This goal is as old as the field of \textit{artificial intelligence} itself \cite{Turing}, and it is a very ambitious one. We shall call this kind of agents "dialog systems".\\

If we think for a second, there is not a single type of conversation. A dialog between strangers in an elevator, a conversation between one psychologist and his patient, a scientific exchange in a conference; all this can be classified as "dialog" but they present very different dynamics and goals. As a way of refining the analysis, the NLP literature on dialog made the distinction between \textit{goal-driven dialog systems} and \textit{non-goal-driven dialog systems}; the former includes \textit{chatbots}, normally used in the industry for technical support services or information acquisition; the latter is a term used to refer to any conversational agent with no explicit purpose.

Although non-goal-driven systems may seen interesting for its comprehensiveness, it presents a central problem: \textit{there is no good quantitative metric to compare non-goal-driven agents}. We can frame the dialog task between two agents A and B as a translation problem: the source language is the set of utterances spoken by A, similarly, the target language is the sentences of B. Then, the dialog system is just a program translating massages from A to B. So it makes sense to consider the use of quantitative metrics for automatic translation, metrics like BLUE \cite{Papineni02bleu:a} and METEOR \cite{Lavie:2007:MAM:1626355.1626389}. But as pointed out by \cite{LiuLSNCP16, LoweSNCP16}, regarding dialog, \textit{these metrics correlate very weakly with human judgement}.

A more fruitful approach is the one from the goal-driven systems literature: developed a series of synthetic tasks in the form of question answering (QA) to test different capabilities of the competing models \cite{BordesW16, Hixon15, WestonBCM15}. Each task tries to assert one prerequisite to full language understanding.

Here we propose to expand the work done in \cite{BordesW16, WestonBCM15} by \textit{adding new testbeds for complex semantic relationships}: \textbf{Entailment-QA}. The motivation behind this expansion is to guarantee that an end-to-end machine learning model can perform \textit{complex linguistic inferences}. We focus on two kind of inferences: the ones defined by \textit{logical operators} and others defined by \textit{word knowledge}.


\section{Motivation}
\label{sec:motivation}

Logic is not important by itself, but it can help us build agents capable of distinguishing between sentences that have a real informational content from sentences that do not. A rational agent should be able to spot contradictions in a sentence. Although this importance, logical reasoning is one area that is often neglected by Conversational AI researchers. This Ph.D. proposal is one step towards a more elaborate analysis.


% ------------------------------------------------------------------------
\section{Objectives}
\label{sec:objectives}

The main objective of this research project is to propose a new set of synthetic tasks in the same lines as \cite{WestonBCM15} in order to help evaluating and building dialog systems that present reasonable text reasoning capabilities.


% ------------------------------------------------------------------------
\section{Organization}
\label{sec:organization}

Chapter 2 exposes the theory that is the basis for the research. Chapter 3 presents our proposed task to evaluate logical reasoning. Chapter 4 describe the proposed methodology. And chapter 5 discuss the possible results of this research.