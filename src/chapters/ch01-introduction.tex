\chapter{Introduction}
\label{ch:01-introduction}

One of the main goals in NLP is to build agents capable of \textit{understanding language and reasoning}, i.e., agents capable of carrying a conversation as if they were human. This goal is as old as the field of \textit{artificial intelligence} itself \cite{Turing}, and it is a very ambitious one. We shall call this kind of agents "dialog systems".

But if we think more carefully about this subject, we will recognize that there is not a single type of conversation. A dialog between strangers in an elevator, a conversation between one psychologist and his patient, a salesman offering a service to a possible client, a scientific exchange in a conference; all these events can be classified as "dialog" but they present very different \textit{dynamics} and \textit{goals}. As a way of refining the analysis, the NLP literature on dialog made the distinction between \textit{goal-driven dialog systems} and \textit{non-goal-driven dialog systems}; the former includes \textit{chatbots}, normally used in the industry for technical support services or information acquisition; the latter is a term used to refer to any conversational agent with no explicit purpose.

Each paradigm presents its advantages and disadvantages: on the one hand goal-driven dialog systems are useful systems but they demand a precise understanding (of the goal at hand) and there are not so much available data to train such systems. On the other hand, non-goal-driven dialog systems produce, at best, chit-chat conversation. Although we have have a lot of available data to train such models (movie legends, social media conversations, etc.), it is not so easy to justify its relevancy. 

Moreover, there is a central difference between these two paradigms: \textit{the evaluation metric}. More precisely, \textit{there is no good quantitative metric to compare non-goal-driven agents}, and as a consequence \textit{there is also no standardized benchmark}. Given this limitations one alternative approach was proposed by \cite{WestonBCM15}: developed a series of synthetic tasks in the form of question answering (QA) to test different capabilities of the competing models. Each task tries to assert one prerequisite to \textit{full language understanding}.

Following this research decision, here we propose to expand the work done in \cite{BordesW16, WestonBCM15} by \textit{adding new testbeds for complex semantic relationships}: \textbf{Entailment-QA}. The motivation behind this expansion is to guarantee that an end-to-end machine learning model can perform \textit{complex linguistic inferences}. So far we have been focusing on two kinds of inferences: the ones defined by \textit{logical operators} and others defined by \textit{word knowledge}.


\section{Motivation}
\label{sec:motivation}

Logic is not important by itself. But it can help us build agents capable of distinguishing between sentences that have a real informational content from sentences that do not. An intelligent agent should distinguish between \textit{meaningful} and \textit{nonsensical} speech. To do that the agent can make use of background knowledge, but it can also point out \textit{gaps in the speech's rationality}. Although this importance, logical reasoning is one area that is often neglected by Conversational AI researchers. This Ph.D. proposal tries to address this deficiency.

We often see a discussion inside the artificial intelligence community between deep learning specialist and the classical IA researcher. This discussion can have a non-productive outcome: we heard that the former's engineering methodology is a kind of unscientific alchemy and that the results from the later has been made irrelevant by big data and deep models. Here we are trying to find a productive middle ground: combining the rich tradition of certain themes of classical AI (like logic reasoning) with the models and techniques of the deep learning community.

% ------------------------------------------------------------------------
\section{Objectives}
\label{sec:objectives}

The goal of this proposal is twofold:  propose a new set of synthetic tasks in the same lines as \cite{WestonBCM15} in order to help evaluating and building dialog systems that present reasonable text reasoning capabilities;  and proposing new models for these tasks. Hence, we have defined the following specific objectives:

\begin{itemize}
\item Create a set of tasks that explicitly make use of complex logical forms.
\item Perform a stress test in the existing models regarding linguistic reasoning.
\item Integrate linguistic reasoning with visual references to create a new set of visual question answering (VQA) tasks.
\item Define new models to achieve better results in the tasks proposed above. 
\end{itemize}

% ------------------------------------------------------------------------
\section{Organization}
\label{sec:organization}

Chapter 2 exposes the theory that is the basis for the research. Chapter 3 presents the models for dialog and our proposed task to evaluate logical reasoning. Chapter 4 describe the proposed methodology. And chapter 5 discuss the possible results of this research.
